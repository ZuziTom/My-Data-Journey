{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming data\n",
    "\n",
    "Data in the data/interim is ready for further processing. Because the format is tabular, we can process it with Pandas. Before proceeding to data analysis, this data must be cleaned:\n",
    "1. From the dataset, discard offers that do not have the amount specified in PLN,\n",
    "2. Change the names of job offers and cities (name and location_city columns) to lower case,\n",
    "3. Add a new salary_avg column as an average of the salary_high and salary_low columns,\n",
    "4. Unify the names of cities:\n",
    "    - e.g. wroclove, wroclaw, wrocław should be changed to wrocław,\n",
    "    - e.g. krakow, kraków +1, kraków +2 should be changed to kraków,\n",
    "    - all other cases that appear should also be cleaned.\n",
    "\n",
    "5. NoFluffJobs does not share information on the country of employment so a column location_city needs to be created or completed:\n",
    "    - if the work is remote, set N/A,\n",
    "    - otherwise, unless the city name suggests otherwise, set PL,\n",
    "Check whether the location_country column is filled correctly\n",
    "\n",
    "6. Add a new column is_senior, that informs whether the position is a senior one or not. To do so use the name of the position: e.g. Senior Data Analyst -> is_senior = 1,\n",
    "\n",
    "7. Save the results to the data\\processed\\ directory, using the ; separator, UTF-8 encoding and without index (index=False). Take the same file name as in task 2 i.e. job_offers_yyyy_mm_dd.csv.\n",
    "\n",
    "# Hints:\n",
    "To unify the names of the cities, use the unique() method and manually correct the rows where the names are incorrect.\n",
    "To check the name of the position for the word \"Senior\", use regular expressions and the re.match method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "                    '...\\\\SESSION 6 WORKSHOP\\\\Phyton_Workshop\\\\data\\\\interim\\\\job_offers_2024_02_11.csv', \n",
    "                    sep=',', # Set up separator based on your need\n",
    "                    decimal=',',\n",
    "                    encoding='utf-8'\n",
    ")\n",
    "\n",
    "print(\"Step 0: Original Table - Shape:\", df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display df\n",
    "print(\"DataFrame:\" )\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract inforamtion from location dictionary column and creating new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# An abstract syntax tree (AST) is a hierarchical representation of the syntactic structure of a Python program. \n",
    "# The 'ast' module allows you to parse Python source code into an AST, manipulate the AST, and generate new Python code from the modified AST.\n",
    "\n",
    "# Creating new columns based on the information in \"location\" nad \"salary\" column\n",
    "\n",
    "### Function to extract location_city\n",
    "def extract_city(location):\n",
    "    location_dict = ast.literal_eval(location) if isinstance(location, str) else {}\n",
    "    return location_dict.get('city', '').lower()\n",
    "\n",
    "# # Apply the function to create 'location_city' column\n",
    "df['location_city'] = df['location'].apply(extract_city)\n",
    "\n",
    "\n",
    "\n",
    "##### Function to extract location_country information\n",
    "def extract_country(location):\n",
    "    location_dict = ast.literal_eval(location) if isinstance(location, str) else {}\n",
    "    return location_dict.get('country', '').lower()\n",
    "\n",
    "# Apply the function to create 'location_country' column\n",
    "df['location_country'] = df['location'].apply(extract_country)\n",
    "\n",
    "\n",
    "\n",
    "##### Function to extract remote information\n",
    "def extract_remotely(location):\n",
    "    location_dict = ast.literal_eval(location) if isinstance(location, str) else {}\n",
    "    return location_dict.get('remotely', '')\n",
    "\n",
    "# Apply the function to create 'location_remote' column\n",
    "df['location_remote'] = df['location'].apply(extract_remotely)\n",
    "\n",
    "print(f\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract of the information from salary column and creating new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function to extract salary_low information\n",
    "def extract_low_salary(salary):\n",
    "    salary_dict = ast.literal_eval(salary) if isinstance(salary, str) else {}\n",
    "    low_value = salary_dict.get('low', '')\n",
    "\n",
    "    # Check if the value is numeric or 'N/A'\n",
    "    if low_value.isdigit():\n",
    "        return float(low_value)\n",
    "    else:\n",
    "        # You can choose to handle non-numeric values in a way that makes sense for your analysis.\n",
    "        # For example, you might return a special value or handle it differently.\n",
    "        return None\n",
    "\n",
    "# Apply the function to create 'salary_high' column\n",
    "df['salary_low'] = df['salary'].apply(extract_low_salary)\n",
    "\n",
    "\n",
    "##### Function to extract salary_lower information\n",
    "def extract_high_salary(salary):\n",
    "    salary_dict = ast.literal_eval(salary) if isinstance(salary, str) else {}\n",
    "    high_value = salary_dict.get('high', '')\n",
    "\n",
    "    # Check if the value is numeric or 'N/A'\n",
    "    if high_value.isdigit():\n",
    "        return float(high_value)\n",
    "    else:\n",
    "        # You can choose to handle non-numeric values in a way that makes sense for your analysis.\n",
    "        # For example, you might return a special value or handle it differently.\n",
    "        return None\n",
    "\n",
    "# Apply the function to create 'salary_low' column\n",
    "df['salary_high'] = df['salary'].apply(extract_high_salary)\n",
    "\n",
    "\n",
    "##### Function to extract salary_currency information\n",
    "def extract_currency(salary):\n",
    "    salary_dict = ast.literal_eval(salary) if isinstance(salary, str) else {}\n",
    "    return salary_dict.get('currency', '')\n",
    "\n",
    "# Apply the function to create 'location_city' column\n",
    "df['salary_currency'] = df['salary'].apply(extract_currency)\n",
    "\n",
    "print(f\"Shape:\", df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Discard offers without the amount specified in PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  Discard offers without the amount specified in PLN / when looking inside dict in very original column\n",
    "\n",
    "# df = df[df['salary'].str.contains(\"'currency': 'PLN'\")]\n",
    "\n",
    "# # Display the filtered table after Step 1\n",
    "# print(f\"Step 1: Discard offers without PLN currency:\", df.shape)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "#######################   Discard (filterout) offers without the amount specified in PLN in currency column\n",
    "df = df[df['salary_currency'] == 'PLN']\n",
    "\n",
    "# Display the filtered table after Step 1\n",
    "print(f\"Step 1: Discard offers without PLN currency:\", df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Change names of job offers and cities to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['name'] = df['name'].str.lower()\n",
    "# df['salary_currency'] = df['salary_currency'].str.lower()\n",
    "\n",
    "df.loc[:, 'name'] = df['name'].str.lower()\n",
    "df.loc[:, 'salary_currency'] = df['salary_currency'].str.lower()\n",
    "\n",
    "\n",
    "# Display the table after Step 2 and its shape\n",
    "print(\"Step 2: Change names to lowercase - Shape:\", df.shape)\n",
    "display(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add a new salary_avg column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['salary_low'] = df['salary_low'].astype(float)\n",
    "# df['salary_high'] = df['salary_high'].astype(float)\n",
    "# df['salary_avg'] = (df['salary_low'] + df['salary_high']) / 2\n",
    "\n",
    "# With loc.\n",
    "\n",
    "df.loc[:, 'salary_low'] = df['salary_low'].astype(float)\n",
    "df.loc[:, 'salary_high'] = df['salary_high'].astype(float)\n",
    "df.loc[:, 'salary_avg'] = (df['salary_low'] + df['salary_high']) / 2\n",
    "\n",
    "# Display the table after Step 3\n",
    "print(f\"Step 3: Add salary_avg column:\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Correct information in Location_city. Remote set to N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'zdalnie' to 'remote' in the 'location_city' column\n",
    "df['location_city'] = df['location_city'].apply(lambda x: 'remote' if 'zdalnie' in str(x).lower() else x)\n",
    "\n",
    "# Rename 'remote' to 'N/A'\n",
    "df['location_city'] = df['location_city'].replace({'remote': 'N/A'}) # Define a function to set location_country based on location_city\n",
    "def set_location_country(row):\n",
    "    if row['location_city'] == 'N/A':\n",
    "        return 'N/A'\n",
    "    else:\n",
    "        return row['location_country']\n",
    "\n",
    "# Apply the function to update location_country\n",
    "df['location_country'] = df.apply(set_location_country, axis=1)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Unify the names of cities by manual correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cities = df['location_city'].unique().astype(str)\n",
    "\n",
    "# Display unique city names and manually correct them\n",
    "for city in unique_cities:\n",
    "    print(f\"Current city name: {city}\")\n",
    "    corrected_name = input(\"Enter the corrected city name (or press Enter to keep it unchanged): \").strip()\n",
    "    \n",
    "    if corrected_name:\n",
    "       df.loc[df['location_city'] == city, 'location_city'] = corrected_name\n",
    "\n",
    "# Display the table after Step 4\n",
    "print(f\"Step 4: Unify city names:\")\n",
    "\n",
    "# Display the resulting DataFrame with unified city names\n",
    "unique_cities = df['location_city'].unique()\n",
    "unique_cities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Add a new column is_senior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "############    LAMBDA\n",
    "# data['is_senior'] = data['name'].apply(lambda x: 1 if re.match(r'.*\\bSenior\\b.*', x) else 0)\n",
    "\n",
    "########### column\n",
    "# def is_senior_position(position_name):\n",
    "#     return 1 if re.match(r'.*\\bSenior\\b.*', position_name, flags=re.IGNORECASE) else 0\n",
    "# df['is_senior'] = df['name'].apply(is_senior_position)\n",
    "\n",
    "########### LOC\n",
    "\n",
    "def is_senior_position(position_name):\n",
    "    return 1 if re.match(r'.*\\bSenior\\b.*', position_name, flags=re.IGNORECASE) else 0\n",
    "\n",
    "# Create a new column 'is_senior' and set values using .loc\n",
    "df['is_senior'] = 0\n",
    "senior_positions = df['name'].apply(is_senior_position) == 1\n",
    "df.loc[senior_positions, 'is_senior'] = 1\n",
    "\n",
    "################## Display the table after Step 6\n",
    "\n",
    "print(f\"\\nStep 6: Add a new column is_senior:\")\n",
    "\n",
    "df.head(2)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning process (duplicates, missing values, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the data before celaning process for later check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'job' column and count the occurrences\n",
    "job_counts = df.groupby('job').size().reset_index(name='count')\n",
    "\n",
    "# Display the resulting DataFrame with counts for each job\n",
    "job_counts\n",
    "\n",
    "print(f\"Number of rows in raw df is {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. cleaning up step - CHECK and DELETE duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom condition for duplicates based on the entire text in specified columns\n",
    "condition = df.duplicated(subset=['name', 'company', 'job', 'location_city', 'location_country', 'location_remote', 'is_senior'], keep=False)\n",
    "\n",
    "# Find and display duplicate rows based on the custom condition\n",
    "duplicates = df[condition]\n",
    "\n",
    "# Display the resulting DataFrame containing duplicates based on the entire text in specified columns\n",
    "print(\"Duplicated Rows:\")\n",
    "\n",
    "duplicates\n",
    "\n",
    "# Print information about the number of duplicate rows\n",
    "print(f\"\\nNumber of duplicated rows: {duplicates.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on the custom condition\n",
    "df_cleaned = df.drop_duplicates(subset=['name', 'company', 'job', 'location_city', 'location_country', 'location_remote', 'is_senior'], keep='first')\n",
    "\n",
    "# Print information about how many rows were deleted\n",
    "deleted_rows = df.shape[0] - df_cleaned.shape[0]\n",
    "print(f\"\\nNumber of rows deleted: {deleted_rows}\")\n",
    "\n",
    "print(f\"df_cleaned has {df_cleaned.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the data after cleaning duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'job' column and count the occurrences\n",
    "job_counts = df_cleaned.groupby('job').size().reset_index(name='count')\n",
    "\n",
    "# Display the resulting DataFrame with counts for each job\n",
    "job_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. cleaning up step - createn new column and filter out jobs where job title and job name {data - analyst, engineer, scientis} dont match. Logic - if job title does not include job name, set up the boolean value FALSE, otherwise TRUE. \n",
    "Print info about number of rows where is_present column value = TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'is_present' and populate it based on the condition (case-insensitive)\n",
    "df['is_present'] = df.apply(lambda row: row['job'].lower() in row['name'].lower(), axis=1)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "df.head(2)\n",
    "\n",
    "# Filter out rows where 'is_present' is False\n",
    "df_final = df_cleaned[df_cleaned['is_present'] == True]  # or df_cleaned[df_cleaned['is_present']]\n",
    "\n",
    "# Display the resulting DataFrame without rows where 'is_present' is False\n",
    "df_final.shape\n",
    "\n",
    "print(f'DF FINAL after filtering match with job and name position has {df_final.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'job' column and count the occurrences\n",
    "job_counts = df_final.groupby('job').size().reset_index(name='count')\n",
    "\n",
    "# Display the resulting DataFrame with counts for each job\n",
    "job_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display df_final\n",
    "print(\"DataFrame after cleanup:\" )\n",
    "print(df_final.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# 7. Save the results to the data\\processed\\ directory\n",
    "processed_dir = '...\\\\SESSION 6 WORKSHOP\\\\Phyton_Workshop\\\\data\\\\processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Generate the file name based on the current date\n",
    "current_date = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "file_name = f'job_offers_{current_date}.csv'\n",
    "file_path = os.path.join(processed_dir, file_name)\n",
    "\n",
    "# Save the DataFrame to CSV with the specified format\n",
    "df_final.to_csv(file_path, sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "print(f\"\\nStep 7: Results saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move ntb to ntbs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def move_notebook_to_notebooks(notebook_path, notebooks_folder):\n",
    "    notebook_filename = os.path.basename(notebook_path)\n",
    "\n",
    "    # Move the notebook file to the \"notebooks\" subfolder\n",
    "    shutil.move(notebook_path, os.path.join(notebooks_folder, notebook_filename))\n",
    "    print(f\"Notebook '{notebook_filename}' moved to the 'notebooks' subfolder.\")\n",
    "\n",
    "# Example usage with specific paths\n",
    "notebook_path = \"...\\\\SESSION 6 WORKSHOP\\\\WEBscraping\\\\3.0_data_transfromation_copy.ipynb\"\n",
    "notebooks_folder = \"...\\\\SESSION 6 WORKSHOP\\\\Phyton_Workshop\\\\notebooks\"\n",
    "\n",
    "move_notebook_to_notebooks(notebook_path, notebooks_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
