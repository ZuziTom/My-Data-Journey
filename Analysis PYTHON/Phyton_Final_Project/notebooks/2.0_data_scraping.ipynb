{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving information about the website\n",
    "Based on the files from the /data/raw directory (generated in exercise 1) extract the following information about an offer:\n",
    "- location - both city and country. For remote work, set Remote as the city and N/A as the country,\n",
    "- salary - both lower and upper limits and currency. If there is no pay range, write the same value in both fields (lower - limit = upper limit),\n",
    "- name of position,\n",
    "- company,\n",
    "- technology.\n",
    "\n",
    "Write the results of a single bid into a dictionary with the following structure:\n",
    "\n",
    "{\n",
    "    'name': 'name of the position',\n",
    "    'company': 'name of the employer',\n",
    "    'technology': 'name of the used technology',\n",
    "    'job': 'information regarding name of the search e.g. data analyst ',\n",
    "    'location': {'city': 'city of employment', 'country': 'country of employment'},\n",
    "    'salary': {'low': 'lower limit', 'high': 'higher limit', 'currency': 'salary currency'} \n",
    "}\n",
    "\n",
    "Put single items into a list.\n",
    "\n",
    "A list of such dictionaries can be read using another Pandas method - json_normalize (documentation link). It is shown during the workshop, because json is a commonly used construct for communication between modules.\n",
    "\n",
    "Save the results as DataFrame to data\\interim\\job_offers.csv using the ; separator, UTF-8 encoding, and without index (index=False).\n",
    "\n",
    "# Complete the exercise following the steps:\n",
    "\n",
    "1. Write a function that takes the HTML code of a page and returns a list with pieces of HTML code that contain information about a single ad,\n",
    "2. Write a function that will take the HTML code containing information about one ad and return a dictionary with the information (described above), \n",
    "3. Assemble this into a working script that Finds all files in the data\\raw directory,\n",
    "    For each file:\n",
    "        - Divides it into sections corresponding to the company,\n",
    "        - Extracts the necessary information from it as a dictionary,\n",
    "        - Will add the dictionary to the previously created list,\n",
    "        - Loads the list with dictionaries using Pandas into the dataset,\n",
    "        - Saves the dataset in the data\\interim\\ directory with the current date.\n",
    "\n",
    "\n",
    "# File names\n",
    "We will adopt the following file naming convention:\n",
    "\n",
    "'job_offers_{current date}.csv'\n",
    "\n",
    "Where the {current date} parameter should use the yyyy_mm_dd format (year month day).\n",
    "\n",
    "# Hints:\n",
    "To get the current date you can use the code: datetime.today().strftime('%Y_%m_%d'). Remember to import the appropriate module!\n",
    "You can split the data parsing for a single offer into several smaller helper functions. For example, one can retrieve the salary, another - parse the location data. This will make the code easier to maintain.\n",
    "To test the performance of your functions, you can manually pull HTML code from a file and pass it as a parameter. This way you don't need the whole script to test how its parts work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract URLs from HTML files and find ¨job name\" for further scraping\n",
    "def extract_url_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the link tag with the specified attributes and extract the 'href' attribute\n",
    "    url_element = soup.find('link', rel='alternate', href=True)\n",
    "    \n",
    "    # Check if the link tag is found before accessing its 'href' attribute\n",
    "    if url_element:\n",
    "        url = url_element['href']\n",
    "        return url\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Specify the directory where your HTML files are stored\n",
    "html_file_path = '...\\\\SESSION 6 WORKSHOP\\\\Phyton_Workshop\\\\data\\\\raw'\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through all files in the directory and print results\n",
    "for filename in os.listdir(html_file_path):\n",
    "    if filename.endswith('.html'):\n",
    "        file_path = os.path.join(html_file_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        url = extract_url_from_html(html_content)\n",
    "        \n",
    "        try:\n",
    "            parsed_url = urllib.parse.urlparse(url)\n",
    "            criteria = urllib.parse.parse_qs(parsed_url.query).get('criteria', [])\n",
    "            job = urllib.parse.unquote(criteria[0]).split('=')[1].replace('%27', '').replace('%20', ' ').strip(\"'\") if criteria and criteria[0] else 'Unknown'\n",
    "            print(f\"File: {filename}\\nURL: {url}\\nExtracted Job name: {job}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing URL for {filename}: {e}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ad_data(html_content):\n",
    "    \"\"\"\n",
    "    Extract ad sections from HTML content.\n",
    "\n",
    "    Parameters:\n",
    "    - html_content (str): HTML content to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of ad sections.\n",
    "    \"\"\"\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    ad = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all 'a' elements with class 'posting-list-item'\n",
    "    ad_sections = ad.find_all('a', class_='posting-list-item')\n",
    "\n",
    "    # Return the found ad sections\n",
    "    return ad_sections\n",
    "\n",
    "\n",
    "def parse_ad_section(html_content, ad_sections):\n",
    "    \"\"\"\n",
    "    Parse ad sections to extract relevant information.\n",
    "\n",
    "    Parameters:\n",
    "    - ad_sections (list): List of ad sections to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of dictionaries containing parsed ad information.\n",
    "    \"\"\"\n",
    "    ad_data = []\n",
    "\n",
    "    # Iterate through each ad section\n",
    "    for ad in ad_sections:\n",
    "        ad_info = {}\n",
    "\n",
    "        # Extract job title\n",
    "        title = ad.find('h3', class_='posting-title__position')\n",
    "        title = title.text.strip() if title else 'N/A'\n",
    "\n",
    "        # Extract company name\n",
    "        company = ad.find('h4', class_='company-name')\n",
    "        company = company.text.strip() if company else 'N/A'\n",
    "\n",
    "        # Extract technologies\n",
    "        technology_elements = ad.find('nfj-posting-item-tiles').find_all('span', class_='tw-text-gray-60')\n",
    "        technologies = [elem.text.strip() for elem in technology_elements if elem.text.strip() not in [' ', '•']]\n",
    "        technology = ', '.join(technologies) if technologies else 'N/A'\n",
    "\n",
    "        # Extract salary\n",
    "        salary = ad.find(\"span\", {\"data-cy\": \"salary ranges on the job offer listing\"})\n",
    "        salary = salary.text.strip().replace('\\xa0', '') if salary else 'N/A'\n",
    "        # Extract salary details\n",
    "        salary_low, salary_high, currency = 'N/A', 'N/A', 'N/A'\n",
    "        salary_parts = salary.split('–')\n",
    "        if len(salary_parts) == 2:\n",
    "            salary_low = salary_parts[0].strip()\n",
    "            salary_high_currency_parts = salary_parts[1].rsplit(maxsplit=1)\n",
    "            salary_high = salary_high_currency_parts[0].strip()\n",
    "            currency = salary_high_currency_parts[1].strip() if len(salary_high_currency_parts) > 1 else ''\n",
    "\n",
    "        # Extract location\n",
    "        location = ad.find('div', class_='tw-flex tw-items-center ng-star-inserted')\n",
    "        location = location.text.strip() if location else 'N/A'\n",
    "        # Extract location details (city, country, remotely)\n",
    "        location_parts = location.split(',')\n",
    "        city = location_parts[0].strip() if len(location_parts) > 0 else 'N/A'\n",
    "        country = location_parts[1].strip() if len(location_parts) > 1 else 'N/A'\n",
    "        remotely = 'Zdalnie' in location\n",
    "\n",
    "        # Check if 'zdialnie' is present in the city, update values accordingly\n",
    "        if 'zdialnie' in city.lower():\n",
    "            city = 'N/A'\n",
    "            country = 'N/A'\n",
    "            remotely = True\n",
    "\n",
    "        # Extract job name from URL\n",
    "        url = extract_url_from_html(html_content)  # Assuming html_content is defined in your context\n",
    "        parsed_url = urllib.parse.urlparse(url)\n",
    "        criteria = urllib.parse.parse_qs(parsed_url.query).get('criteria', [])\n",
    "        job = urllib.parse.unquote(criteria[0]).split('=')[1].replace('%27', '').replace('%20', ' ').strip(\"'\") if criteria and criteria[0] else 'Unknown'\n",
    "\n",
    "        # Populate ad_info dictionary\n",
    "        ad_info = {\n",
    "            'title': title,\n",
    "            'company': company,\n",
    "            'technology': technology,\n",
    "            'job': job,\n",
    "            'location': {'city': city, 'country': country, 'remotely': remotely},\n",
    "            'salary': {'low': salary_low, 'high': salary_high, 'currency': currency}\n",
    "        }\n",
    "\n",
    "        # Append ad_info to ad_data list\n",
    "        ad_data.append(ad_info)\n",
    "\n",
    "    # Return the list of parsed ad information\n",
    "    return ad_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "html_file_path = '...\\\\SESSION 6 WORKSHOP\\\\Phyton_Workshop\\\\data\\\\raw\\\\data engineer_5.html'\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Extract ad sections using the provided HTML content\n",
    "ads = extract_ad_data(html_content)\n",
    "\n",
    "# Print the number of ad sections found\n",
    "print(\"Number of ad sections:\", len(ads))\n",
    "\n",
    "# Parse the ad sections to extract relevant information\n",
    "parsed_ads = parse_ad_section(html_content, ads)  # Pass html_content to parse_ad_section\n",
    "\n",
    "# Print the parsed ad information\n",
    "print(parsed_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(directory_path):\n",
    "    \"\"\"\n",
    "    Process all files in a directory and extract ad information.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path (str): Path to the directory containing HTML files.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of dictionaries containing parsed ad information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the extracted ad information\n",
    "    ads_list = []\n",
    "\n",
    "    # Iterate through each file in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Check if the file has a '.html' extension\n",
    "        if filename.endswith('.html'):\n",
    "            # Construct the full path to the file\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            # Read the HTML content from the file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "\n",
    "            # Print a message indicating the current file being processed\n",
    "            print(f'Processing file: {filename}')\n",
    "\n",
    "            # Extract ad sections from the HTML content\n",
    "            ads = extract_ad_data(html_content)\n",
    "\n",
    "            # Parse ad sections to extract relevant information\n",
    "            parsed_ads = parse_ad_section(html_content, ads)\n",
    "\n",
    "            # Extend the ads_list with the parsed information from the current file\n",
    "            ads_list.extend(parsed_ads)\n",
    "\n",
    "            # Print the number of ads found in the current file\n",
    "            print(f'Number of ads in {filename}: {len(parsed_ads)}')\n",
    "\n",
    "    # Return the list of parsed ad information for all files\n",
    "    return ads_list\n",
    "\n",
    "# Specify the directory where your HTML files are stored\n",
    "html_directory_path = '...\\\\SESSION 6 WORKSHOP\\\\Phyton_Workshop\\\\data\\\\raw'\n",
    "\n",
    "# Call the process_files function and store the result in a variable\n",
    "result_ads_list = process_files(html_directory_path)\n",
    "\n",
    "# Print the resulting list of parsed ad information\n",
    "print(\"Final List of Parsed Ads:\")\n",
    "for idx, ad_info in enumerate(result_ads_list, 1):\n",
    "    print(f\"Ad {idx}:\", ad_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Function to save data to CSV file with the current date\n",
    "def save_to_csv(data, output_directory):\n",
    "    current_date = datetime.today().strftime('%Y_%m_%d')\n",
    "    output_file_path = os.path.join(output_directory, f'job_offers_{current_date}.csv')\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(f'Dataset saved to {output_file_path}')\n",
    "\n",
    "# Specify the input directory where your HTML files are processed\n",
    "input_directory = '...\\\\Phyton_Workshop\\\\data\\\\raw_test'\n",
    "\n",
    "# Specify the output directory where you want to save the CSV file\n",
    "output_directory = '...\\\\SESSION 6 WORKSHOP\\\\Phyton_Workshop\\\\data\\\\interim_test'\n",
    "\n",
    "# Call the save_to_csv function with the correct parameters\n",
    "save_to_csv(result_ads_list, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def move_notebook_to_notebooks(notebook_path, notebooks_folder):\n",
    "    notebook_filename = os.path.basename(notebook_path)\n",
    "\n",
    "    # Move the notebook file to the \"notebooks\" subfolder\n",
    "    shutil.move(notebook_path, os.path.join(notebooks_folder, notebook_filename))\n",
    "    print(f\"Notebook '{notebook_filename}' moved to the 'notebooks' subfolder.\")\n",
    "\n",
    "# Example usage with specific paths\n",
    "notebook_path = \"...\\\\SESSION 6 WORKSHOP\\\\WEBscraping\\\\2.scraping data.ipynb\"\n",
    "notebooks_folder = \"...\\\\SESSION 6 WORKSHOP\\\\Phyton_Workshop\\\\notebooks\"\n",
    "\n",
    "move_notebook_to_notebooks(notebook_path, notebooks_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
